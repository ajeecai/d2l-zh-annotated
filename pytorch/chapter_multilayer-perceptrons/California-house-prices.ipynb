{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import hashlib\n",
    "import requests\n",
    "\n",
    "cache_dir = './data'\n",
    "def download_data(file, hash) :\n",
    "    url = \"https://p.cloudgav.com/\"+file\n",
    "    fname = os.path.join(cache_dir, url.split('/')[-1])\n",
    "    print(f'file in {fname}')\n",
    "    dl = False\n",
    "    os.makedirs(cache_dir, exist_ok=True)\n",
    "\n",
    "    if os.path.exists(fname):\n",
    "        with open(fname, 'r+b') as f:\n",
    "            content = f.read()\n",
    "            if len(content ) == 0:\n",
    "                print('empty file')\n",
    "                dl = True\n",
    "            else:\n",
    "                sha256 = hashlib.sha256()\n",
    "                sha256.update(content)\n",
    "                print(f\"sha256 is {sha256.hexdigest()}\")\n",
    "                if hash != sha256.hexdigest():\n",
    "                    f.truncate(0)\n",
    "                    dl = True\n",
    "                else:\n",
    "                    print(\"file is good, no need to re-download\")\n",
    "    else:\n",
    "        dl = True\n",
    "\n",
    "    if dl == True:\n",
    "        print(f\"Download to {fname}\")\n",
    "        with open(fname, 'w+b') as f:\n",
    "            r = requests.get(url, stream=True, verify=True)\n",
    "            if r.status_code != 200:\n",
    "                raise SystemExit(f\"status error {r.status_code}!\")\n",
    "\n",
    "            # assuming the download is always correct\n",
    "            f.write(r.content)\n",
    "            print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file in ./data/train.csv.zip\n",
      "sha256 is 1bac0c0bcbbbd4965a89edbe2dc632ab875d2f696b8a3c83f93fe1707666601d\n",
      "file is good, no need to re-download\n",
      "file in ./data/test.csv.zip\n",
      "sha256 is caf72c51b825ed5481bd0df949f889b87266879dfd567fc6dd0bd6db095d4606\n",
      "file is good, no need to re-download\n"
     ]
    }
   ],
   "source": [
    "download_data(\"train.csv.zip\", \"1bac0c0bcbbbd4965a89edbe2dc632ab875d2f696b8a3c83f93fe1707666601d\")\n",
    "download_data(\"test.csv.zip\", \"caf72c51b825ed5481bd0df949f889b87266879dfd567fc6dd0bd6db095d4606\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read data in, then filter out the features which are not useful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import pandas as pd\n",
    "\n",
    "train_zip = zipfile.ZipFile(os.path.join(cache_dir, \"train.csv.zip\"))\n",
    "test_zip = zipfile.ZipFile(os.path.join(cache_dir, \"test.csv.zip\"))\n",
    "\n",
    "train_zip.extractall(cache_dir)\n",
    "test_zip.extractall(cache_dir)\n",
    "\n",
    "train_df = pd.read_csv(os.path.join(cache_dir, \"train.csv\"))\n",
    "test_df = pd.read_csv(os.path.join(cache_dir, \"test.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(47439, 41) ['Address', 'Annual tax amount', 'Appliances included', 'Bathrooms', 'Bedrooms', 'City', 'Cooling', 'Cooling features', 'Elementary School', 'Elementary School Distance', 'Elementary School Score', 'Flooring', 'Full bathrooms', 'Garage spaces', 'Heating', 'Heating features', 'High School', 'High School Distance', 'High School Score', 'Id', 'Last Sold On', 'Last Sold Price', 'Laundry features', 'Listed On', 'Listed Price', 'Lot', 'Middle School', 'Middle School Distance', 'Middle School Score', 'Parking', 'Parking features', 'Region', 'Sold Price', 'State', 'Summary', 'Tax assessed value', 'Total interior livable area', 'Total spaces', 'Type', 'Year built', 'Zip']\n",
      "(31626, 40) ['Address', 'Annual tax amount', 'Appliances included', 'Bathrooms', 'Bedrooms', 'City', 'Cooling', 'Cooling features', 'Elementary School', 'Elementary School Distance', 'Elementary School Score', 'Flooring', 'Full bathrooms', 'Garage spaces', 'Heating', 'Heating features', 'High School', 'High School Distance', 'High School Score', 'Id', 'Last Sold On', 'Last Sold Price', 'Laundry features', 'Listed On', 'Listed Price', 'Lot', 'Middle School', 'Middle School Distance', 'Middle School Score', 'Parking', 'Parking features', 'Region', 'State', 'Summary', 'Tax assessed value', 'Total interior livable area', 'Total spaces', 'Type', 'Year built', 'Zip']\n"
     ]
    }
   ],
   "source": [
    "train_cols = sorted(train_df.columns)\n",
    "test_cols = sorted(test_df.columns)\n",
    "\n",
    "print(train_df.shape, train_cols)\n",
    "print(test_df.shape,test_cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib_inline import backend_inline\n",
    "from IPython import display\n",
    "import numpy as np\n",
    "\n",
    "class AnimatedLoss:\n",
    "    \"\"\"在动画中绘制数据\"\"\"\n",
    "    def __init__(self, subplots_config, nrows=1, ncols=1, figsize=(3.5, 2.5)):\n",
    "        self.use_svg_display()\n",
    "        self.fig, self.axes = plt.subplots(nrows, ncols, figsize=figsize)\n",
    "        if nrows * ncols == 1:\n",
    "            self.axes = [self.axes]\n",
    "        elif isinstance(self.axes, np.ndarray):\n",
    "            self.axes = self.axes.flatten().tolist()\n",
    "        self.subplots_config = subplots_config\n",
    "        self.lines = [[] for _ in range(len(self.axes))]\n",
    "        self._initialize_axes()\n",
    "\n",
    "    @staticmethod\n",
    "    def set_axes(axes, xlabel, ylabel, xlim, ylim, xscale, yscale, legend):\n",
    "        axes.set_xlabel(xlabel)\n",
    "        axes.set_ylabel(ylabel)\n",
    "        axes.set_xscale(xscale)\n",
    "        axes.set_yscale(yscale)\n",
    "        axes.set_xlim(xlim)\n",
    "        axes.set_ylim(ylim)\n",
    "        if legend:\n",
    "            axes.legend(legend)\n",
    "        axes.grid()\n",
    "\n",
    "    @staticmethod\n",
    "    def use_svg_display():\n",
    "        backend_inline.set_matplotlib_formats('svg')\n",
    "\n",
    "    def _initialize_axes(self):\n",
    "        for ax, config in zip(self.axes, self.subplots_config):\n",
    "            self.set_axes(\n",
    "                ax, \n",
    "                config.get('xlabel', None), \n",
    "                config.get('ylabel', None), \n",
    "                config.get('xlim', None), \n",
    "                config.get('ylim', None), \n",
    "                config.get('xscale', 'linear'), \n",
    "                config.get('yscale', 'linear'), \n",
    "                config.get('legend', None)\n",
    "            )\n",
    "\n",
    "    def add(self, x, y, subplot_index=0):\n",
    "        config = self.subplots_config[subplot_index]\n",
    "        if not isinstance(y, list):\n",
    "            y = [y]\n",
    "        n = len(y)\n",
    "        if not hasattr(x, \"__len__\"):\n",
    "            x = [x] * n\n",
    "        if not self.lines[subplot_index]:\n",
    "            self.lines[subplot_index] = []\n",
    "            for fmt, label in zip(config.get('fmts', ['-']), config.get('legend', [None]*n)):\n",
    "                line, = self.axes[subplot_index].plot([], [], fmt, label=label)\n",
    "                self.lines[subplot_index].append(line)\n",
    "\n",
    "        for line, yi in zip(self.lines[subplot_index], y):\n",
    "            new_x = np.append(line.get_xdata(), x)\n",
    "            new_y = np.append(line.get_ydata(), yi)\n",
    "            line.set_data(new_x, new_y)\n",
    "        \n",
    "        self.axes[subplot_index].relim()  # Recompute limits\n",
    "        self.axes[subplot_index].autoscale_view()  # Autoscale\n",
    "        self.axes[subplot_index].legend() \n",
    "\n",
    "        display.display(self.fig)\n",
    "        display.clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop not useful columns\n",
    "TODO: please also remove multicollinearity, such as \"Parking\", and \"Parking features\", or combine them into one, and \"State\", \"City\", \"Region\" can be represented by Zip?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 spaces <===> nan\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.10069773814793735"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check how is the difference between \"Parking\" and \"Parking features\"\n",
    "# pd.set_option('display.max_row', None)\n",
    "(train_df['Parking'] == train_df[\"Parking features\"]).sum()\n",
    "print(train_df['Parking'][2], \"<===>\", train_df['Parking features'][2])\n",
    "\n",
    "mask = (train_df['Parking'] != train_df['Parking features'])\n",
    "mask.sum()/train_df.shape[0]\n",
    "# for parking, parking_feature in zip(train_df[mask][\"Parking\"], train_df[mask][\"Parking features\"]):\n",
    "#     print(parking, \"<===>\",parking_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "import re\n",
    "\n",
    "def clean_string(s):\n",
    "    s = s.lower()  # Strip and convert to lower case\n",
    "    s = re.sub(r'\\-+', ' ', s)  # Replace multiple - with a single space\n",
    "    s = re.sub(r'\\s+', ' ', s)  # Replace multiple spaces with a single space\n",
    "    return s\n",
    "\n",
    "def expand_col(df, col_name):\n",
    "    col = df[col_name]\n",
    "    col.fillna(\"0\", inplace=True)\n",
    "    # col is a pandas Series objects, so apply is working on its each element\n",
    "    col = col.apply(lambda s: [e.strip() for e in clean_string(s).split(',')])\n",
    "    # print(col)\n",
    "    unique = set()\n",
    "    for val in col:\n",
    "        unique.update(val)\n",
    "        # print(f\"val is {val}, unique is {unique}\")\n",
    "\n",
    "    new_columns = pd.DataFrame()    \n",
    "    for val in unique:\n",
    "        new_columns= pd.concat([new_columns,col.apply(lambda x: int(val in x) ).to_frame(name=f'{col_name}_{val}')], axis=1)\n",
    "    \n",
    "    # print(new_columns.shape)\n",
    "    \n",
    "    df = df.join(new_columns)\n",
    "    \n",
    "    df.drop(col_name, axis=1, inplace=True)\n",
    "    return df\n",
    "\n",
    "def expand_col_date(df, col_name):\n",
    "    col = df[col_name]\n",
    "    col.fillna(\"0\", inplace=True)\n",
    "    # after read_cvs, xx/yy/zz turned into xx-yy-zz, zz is not important\n",
    "    new_columns = pd.DataFrame()    \n",
    "\n",
    "    new_columns[f'{col_name}_year'] = col.apply(lambda x: int(x.split('-')[0]) if '-' in x  else 0)\n",
    "    new_columns[f'{col_name}_month'] = col.apply(lambda x: int(x.split('-')[1]) if '-' in x and len(x.split('-')) > 1 else 0)\n",
    "    \n",
    "    df = df.join(new_columns)\n",
    "\n",
    "    df.drop(col_name, axis=1, inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "def print_col_start_with(name):\n",
    "    expand = train_df.filter(regex=f'^{name}').columns\n",
    "    pprint.pprint(f'{name}_expand to {len(expand)}')\n",
    "    pprint.pprint(expand)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean up the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Id                               int64\n",
      "Address                         object\n",
      "Sold Price                     float64\n",
      "Summary                         object\n",
      "Type                            object\n",
      "Year built                     float64\n",
      "Heating                         object\n",
      "Cooling                         object\n",
      "Parking                         object\n",
      "Lot                            float64\n",
      "Bedrooms                        object\n",
      "Bathrooms                      float64\n",
      "Full bathrooms                 float64\n",
      "Total interior livable area    float64\n",
      "Total spaces                   float64\n",
      "Garage spaces                  float64\n",
      "Region                          object\n",
      "Elementary School               object\n",
      "Elementary School Score        float64\n",
      "Elementary School Distance     float64\n",
      "Middle School                   object\n",
      "Middle School Score            float64\n",
      "Middle School Distance         float64\n",
      "High School                     object\n",
      "High School Score              float64\n",
      "High School Distance           float64\n",
      "Flooring                        object\n",
      "Heating features                object\n",
      "Cooling features                object\n",
      "Appliances included             object\n",
      "Laundry features                object\n",
      "Parking features                object\n",
      "Tax assessed value             float64\n",
      "Annual tax amount              float64\n",
      "Listed On                       object\n",
      "Listed Price                   float64\n",
      "Last Sold On                    object\n",
      "Last Sold Price                float64\n",
      "City                            object\n",
      "Zip                              int64\n",
      "State                           object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(train_df.dtypes)\n",
    "# print(train_df.dtypes[train_df.dtypes != 'object'])\n",
    "# 在Pandas中，当你用一个布尔Series去索引DataFrame时，它通常是用来选择行的，而不是列\n",
    "# 因为train_df.dtypes 是行的形式：\n",
    "# Id                                  int64\n",
    "# Address                            object\n",
    "# Sold Price                        float64\n",
    "# Summary                            object\n",
    "# Type                               object\n",
    "#\n",
    "# 所以，要选择列features，需要用这种形式来选取列的名字列表：                              ...   \n",
    "numeric_features = train_df.dtypes[train_df.dtypes != 'object'].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data's shape xx (47439, 41)\n",
      "\"train data's shape after get dummy (47439, 3562)\"\n"
     ]
    }
   ],
   "source": [
    "print(f\"train data's shape xx\", train_df.shape)\n",
    "train_df.drop('Summary', axis=1, inplace=True)\n",
    "train_df.drop('Id', axis=1, inplace=True)\n",
    "train_df.drop('Address', axis=1, inplace=True) # zip is enough\n",
    "train_df.drop('Heating', axis=1, inplace=True) # Heating features is enough\n",
    "train_df.drop('Cooling', axis=1, inplace=True) # Cooling features is enough\n",
    "train_df.drop('Parking', axis=1,inplace=True)  # Parking features is enough\n",
    "train_df.drop('Region', axis=1,inplace=True)  # Parking features is enough\n",
    "train_df.drop('State', axis=1,inplace=True)  # Parking features is enough\n",
    "train_df.drop('City', axis=1,inplace=True)  # Parking features is enough\n",
    "\n",
    "numeric_features = train_df.dtypes[train_df.dtypes != 'object'].index\n",
    "numeric_features_not_price = [feature for feature in numeric_features if feature != \"Sold Price\" ]\n",
    "\n",
    "train_df[numeric_features] = train_df[numeric_features].astype(float)\n",
    "# don't normalized for labels\n",
    "train_df[numeric_features_not_price] = train_df[numeric_features_not_price].apply(\n",
    "    lambda x: (x - x.mean()) / (x.std()))\n",
    "\n",
    "# TODO: fillna with x.mean\n",
    "train_df[numeric_features] = train_df[numeric_features].fillna(0)\n",
    "\n",
    "train_df = expand_col(train_df,\"Appliances included\")\n",
    "train_df = expand_col(train_df,\"Heating features\")\n",
    "train_df = expand_col(train_df,\"Cooling features\")\n",
    "train_df = expand_col(train_df,\"Parking features\")\n",
    "train_df = expand_col(train_df,\"Flooring\")\n",
    "train_df = expand_col(train_df,\"Laundry features\")\n",
    "train_df = expand_col(train_df,\"Bedrooms\")\n",
    "train_df = expand_col(train_df,\"Type\")\n",
    "\n",
    "train_df = expand_col_date(train_df, \"Listed On\")\n",
    "train_df = expand_col_date(train_df, \"Last Sold On\")\n",
    "\n",
    "train_df = pd.get_dummies(train_df, dummy_na=True)\n",
    "# TODO: why TRUE or FALSE instead of 0/1 unless calling astype\n",
    "train_df = train_df.astype(float)\n",
    "\n",
    "# print_col_start_with('Type')\n",
    "# print(train_df[\"Listed On_year\"])\n",
    "# pprint.pprint(train_df.columns.to_list())\n",
    "\n",
    "\n",
    "pprint.pprint(f\"train data's shape after get dummy {train_df.shape}\")\n",
    "train_df[0:10].to_csv('cleanup_data.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found NVIDIA GeForce RTX 2080 Ti\n",
      "using device: cuda:0\n",
      "675911168\n",
      "train_data_tensor dtype torch.float32\n",
      "675911168\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "dev_id = 0\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"found {torch.cuda.get_device_name(dev_id)}\")\n",
    "    device = torch.device(f\"cuda:{dev_id}\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    \n",
    "print(f\"using device: {device}\")\n",
    "\n",
    "print(torch.cuda.memory_allocated())\n",
    "\n",
    "\n",
    "train_data_tensor = torch.tensor(train_df.drop(\"Sold Price\",axis=1).values, dtype=torch.float32).to(device)\n",
    "train_label_tensor = torch.tensor(train_df[\"Sold Price\"].values, dtype=torch.float32).to(device)\n",
    "print(f\"train_data_tensor dtype {train_data_tensor.dtype}\")\n",
    "\n",
    "print(torch.cuda.memory_allocated())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "split data into k-fold, one for verification and rest for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "# TODO: make a a real k fold instead of random selection\n",
    "def k_data_split(data_len, k):\n",
    "    if k >= data_len or k < 3:\n",
    "        raise SystemExit(f\"k is too big or too small {k}\")\n",
    "    # print(f\"data_len {data_len}\")\n",
    "    k_len = data_len//k\n",
    "    index = list(range(0,data_len))\n",
    "    random.shuffle(index)\n",
    "    return index[0:k_len], index[k_len:]\n",
    "\n",
    "\n",
    "# K = 10\n",
    "# k_verify_df,k_train_df = k_data_split(train_df, K)\n",
    "# print(f\"k_verify_df len {len(k_verify_df)}, k_train_df len {len(k_train_df)}\")\n",
    "# print(f\"k_train_df 0-9 \", k_train_df[0:10].to_string())\n",
    "# print(f\"k_verify_df 0-9 \", k_verify_df[0:10].to_string())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=3561, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch import Tensor, nn\n",
    "import torch\n",
    "import warnings\n",
    "\n",
    "class LogMseLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    # forward will be automatically called when this class is from nn.Module\n",
    "    def forward(self, predicted, target):\n",
    "        if not (predicted.size() == target.size()):\n",
    "            warnings.warn(\n",
    "                f\"Using a target size ({target.size()}) that is different to the input size ({input.size()}). \"\n",
    "                \"This will likely lead to incorrect results due to broadcasting. \"\n",
    "                \"Please ensure they have the same size.\",\n",
    "                stacklevel=2,\n",
    "            )\n",
    "        \n",
    "        predicted_mod = torch.clamp(predicted, 1)\n",
    "        target_mod = torch.clamp(target, 1)\n",
    "        r = torch.sqrt(torch.mean((torch.log(predicted_mod) - torch.log(target_mod)) ** 2))\n",
    "        # print(f\"log loss {r}, shape {r.shape}, predicted shape {predicted.shape}: {predicted[0:3]}, target shape {target.shape}: {target[0:3]}\")\n",
    "        return r\n",
    "\n",
    "log_mse_loss = LogMseLoss()\n",
    "\n",
    "loss = nn.MSELoss()\n",
    "\n",
    "def nn_net():\n",
    "    feature_size = train_df.shape[1]-1 # -1 to remove the label column\n",
    "    return nn.Sequential(nn.Linear(feature_size,1))\n",
    "    # return nn.Sequential(nn.Linear(feature_size,feature_size//2),\n",
    "    #                      nn.ReLU(),\n",
    "    #                      nn.Linear(feature_size//2, 1)) \n",
    "def init_xavier(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.xavier_uniform_(m.weight)\n",
    "\n",
    "def init_kaiming(m):\n",
    "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "        nn.init.kaiming_uniform_(m.weight, mode='fan_in', nonlinearity='relu')\n",
    "        if m.bias is not None:\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "\n",
    "net = nn_net()\n",
    "net.apply(init_kaiming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 675911168\n",
      "1356857344\n"
     ]
    }
   ],
   "source": [
    "print(\"1\", torch.cuda.memory_allocated())\n",
    "print(torch.cuda.memory_reserved())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader,Dataset, TensorDataset\n",
    "import numpy as np\n",
    "\n",
    "# Convert DataFrame to a PyTorch Dataset\n",
    "class CustomDataset(TensorDataset):\n",
    "    def __init__(self, data_tensor, label_tensor):\n",
    "        # print(dataframe.apply(pd.to_numeric, errors='coerce').isnull())\n",
    "        self.features = data_tensor\n",
    "        # print(f\"features dtype {self.features.dtype}\")\n",
    "        self.labels = label_tensor.reshape(self.features.shape[0], -1)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx], self.labels[idx]\n",
    "\n",
    "K = 10\n",
    "epochs = 50\n",
    "batch_size = 32\n",
    "lr = 1e-3\n",
    "weight_decay = 0\n",
    "train_loss_mean, verify_loss_mean, train_loss_array, verify_loss_array = np.zeros((epochs)), np.zeros((epochs)), \\\n",
    "                                                np.zeros((K,epochs)), np.zeros((K,epochs)) \n",
    "\n",
    "net = net.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(net.parameters(),\n",
    "                                 lr = lr,\n",
    "                                 weight_decay = weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "675926016\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.memory_allocated())\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ===>   K:0 675926016\n",
      "   reserved 1356857344\n",
      "         1: 676306432\n",
      "   reserved 1356857344\n",
      "         2: 676306432\n",
      "   reserved 1356857344\n",
      "         3: 676306432\n",
      "   reserved 1356857344\n",
      "       ep0: 676466176\n",
      "  reserved: 1356857344\n",
      "       ep1: 676657664\n",
      "  reserved: 1356857344\n",
      "       ep2: 676657664\n",
      "  reserved: 1356857344\n",
      "       ep3: 676657664\n",
      "  reserved: 1356857344\n",
      "       ep4: 676657664\n",
      "  reserved: 1356857344\n",
      "       ep5: 676657664\n",
      "  reserved: 1356857344\n",
      "       ep6: 676657664\n",
      "  reserved: 1356857344\n",
      "       ep7: 676657664\n",
      "  reserved: 1356857344\n",
      "       ep8: 676657664\n",
      "  reserved: 1356857344\n",
      "       ep9: 676657664\n",
      "  reserved: 1356857344\n",
      "       ep10: 676657664\n",
      "  reserved: 1356857344\n",
      "       ep11: 676657664\n",
      "  reserved: 1356857344\n",
      "       ep12: 676657664\n",
      "  reserved: 1356857344\n",
      "       ep13: 676657664\n",
      "  reserved: 1356857344\n",
      "       ep14: 676657664\n",
      "  reserved: 1356857344\n",
      "       ep15: 676657664\n",
      "  reserved: 1356857344\n",
      "       ep16: 676657664\n",
      "  reserved: 1356857344\n",
      "       ep17: 676657664\n",
      "  reserved: 1356857344\n",
      "       ep18: 676657664\n",
      "  reserved: 1356857344\n",
      "       ep19: 676657664\n",
      "  reserved: 1356857344\n",
      "       ep20: 676657664\n",
      "  reserved: 1356857344\n",
      "       ep21: 676657664\n",
      "  reserved: 1356857344\n",
      "       ep22: 676657664\n",
      "  reserved: 1356857344\n",
      "       ep23: 676657664\n",
      "  reserved: 1356857344\n",
      "       ep24: 676657664\n",
      "  reserved: 1356857344\n",
      "       ep25: 676657664\n",
      "  reserved: 1356857344\n",
      "       ep26: 676657664\n",
      "  reserved: 1356857344\n",
      "       ep27: 676657664\n",
      "  reserved: 1356857344\n",
      "       ep28: 676657664\n",
      "  reserved: 1356857344\n",
      "       ep29: 676657664\n",
      "  reserved: 1356857344\n",
      "       ep30: 676657664\n",
      "  reserved: 1356857344\n",
      "       ep31: 676657664\n",
      "  reserved: 1356857344\n",
      "       ep32: 676657664\n",
      "  reserved: 1356857344\n",
      "       ep33: 676657664\n",
      "  reserved: 1356857344\n",
      "       ep34: 676657664\n",
      "  reserved: 1356857344\n",
      "       ep35: 676657664\n",
      "  reserved: 1356857344\n",
      "       ep36: 676657664\n",
      "  reserved: 1356857344\n",
      "       ep37: 676657664\n",
      "  reserved: 1356857344\n",
      "       ep38: 676657664\n",
      "  reserved: 1356857344\n",
      "       ep39: 676657664\n",
      "  reserved: 1356857344\n",
      "       ep40: 676657664\n",
      "  reserved: 1356857344\n",
      "       ep41: 676657664\n",
      "  reserved: 1356857344\n",
      "       ep42: 676657664\n",
      "  reserved: 1356857344\n",
      "       ep43: 676657664\n",
      "  reserved: 1356857344\n",
      "       ep44: 676657664\n",
      "  reserved: 1356857344\n",
      "       ep45: 676657664\n",
      "  reserved: 1356857344\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 44\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ep \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;66;03m# print(f'\\t\\tepoch:{ep}')\u001b[39;00m\n\u001b[1;32m     43\u001b[0m     j \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m---> 44\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m X, y \u001b[38;5;129;01min\u001b[39;00m train_data:\n\u001b[1;32m     45\u001b[0m         \u001b[38;5;66;03m# print(f\"X shape, {X.shape} {X.dtype}, y shape {y.shape}\")\u001b[39;00m\n\u001b[1;32m     46\u001b[0m         optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     47\u001b[0m         l \u001b[38;5;241m=\u001b[39m loss(net(X),y)\n",
      "File \u001b[0;32m~/.conda/envs/d2l-mine/lib/python3.9/site-packages/torch/utils/data/dataloader.py:652\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    649\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    650\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    651\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 652\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    653\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    654\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    655\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    656\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/.conda/envs/d2l-mine/lib/python3.9/site-packages/torch/utils/data/dataloader.py:692\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    691\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 692\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    693\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    694\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/.conda/envs/d2l-mine/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 52\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/d2l-mine/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py:142\u001b[0m, in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    140\u001b[0m         out \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39mnew(storage)\u001b[38;5;241m.\u001b[39mresize_(\u001b[38;5;28mlen\u001b[39m(batch), \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlist\u001b[39m(elem\u001b[38;5;241m.\u001b[39msize()))\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mstack(batch, \u001b[38;5;241m0\u001b[39m, out\u001b[38;5;241m=\u001b[39mout)\n\u001b[0;32m--> 142\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[43melem_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__module__\u001b[39;49m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnumpy\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m elem_type\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstr_\u001b[39m\u001b[38;5;124m'\u001b[39m \\\n\u001b[1;32m    143\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m elem_type\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstring_\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    144\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m elem_type\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mndarray\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m elem_type\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmemmap\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    145\u001b[0m         \u001b[38;5;66;03m# array of string classes and object\u001b[39;00m\n\u001b[1;32m    146\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m np_str_obj_array_pattern\u001b[38;5;241m.\u001b[39msearch(elem\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mstr) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(K):\n",
    "    # subplots_config = [\n",
    "    #     {'xlabel': 'epoch', 'ylabel': 'loss', 'xlim': (0, epochs), 'ylim': (1.1, 1.6), 'xscale': 'linear', 'yscale': 'linear', 'legend': ['train', 'verify'], 'fmts': ['r-', 'b--']}\n",
    "    # ]\n",
    "\n",
    "    # animator = AnimatedLoss(subplots_config, nrows=1, ncols=1, figsize=(10, 5) )\n",
    "    print(f' ===>   K:{i}', torch.cuda.memory_allocated())\n",
    "    print(f'   reserved', torch.cuda.memory_reserved())    \n",
    "    \n",
    "    # print(f' ===> K:{i}')\n",
    "    k_verify_index,k_train_index = k_data_split(len(train_data_tensor), K)\n",
    "    k_verify_index,k_train_index = torch.tensor(k_verify_index).to(device),torch.tensor(k_train_index).to(device)\n",
    "    # print(f\"train data len {len(k_train_index)}, verify data len {len(k_verify_index)}\")\n",
    "    print(f'         1:', torch.cuda.memory_allocated())\n",
    "    print(f'   reserved', torch.cuda.memory_reserved())    \n",
    "\n",
    "    # k_train_data_tensor = train_data_tensor.index_select(0, k_train_index)\n",
    "    # k_train_label_tensor = train_label_tensor.index_select(0, k_train_index)\n",
    "    # k_verify_data_tensor = train_data_tensor.index_select(0, k_verify_index)\n",
    "    # k_verify_label_tensor = train_label_tensor.index_select(0, k_verify_index)\n",
    "\n",
    "    k_train_data_tensor = train_data_tensor[len(train_data_tensor)//K:]\n",
    "    k_train_label_tensor = train_label_tensor[len(train_data_tensor)//K:]\n",
    "    k_verify_data_tensor = train_data_tensor[0:len(train_data_tensor)//K:]\n",
    "    k_verify_label_tensor = train_label_tensor[0:len(train_data_tensor)//K:]\n",
    "\n",
    "    print(f'         2:', torch.cuda.memory_allocated())\n",
    "    print(f'   reserved', torch.cuda.memory_reserved())\n",
    "\n",
    "\n",
    "    # print(f\"len of k_train_data_tensor {len(k_train_data_tensor)}\")\n",
    "\n",
    "    train_data = DataLoader(CustomDataset(k_train_data_tensor,k_train_label_tensor), shuffle=True, batch_size = batch_size)\n",
    "    \n",
    "\n",
    "    print(f'         3:', torch.cuda.memory_allocated())\n",
    "    print(f'   reserved', torch.cuda.memory_reserved())    \n",
    "\n",
    "\n",
    "    j = -1\n",
    "    for ep in range(epochs):\n",
    "        # print(f'\\t\\tepoch:{ep}')\n",
    "        j += 1\n",
    "        for X, y in train_data:\n",
    "            # print(f\"X shape, {X.shape} {X.dtype}, y shape {y.shape}\")\n",
    "            optimizer.zero_grad()\n",
    "            l = loss(net(X),y)\n",
    "            l.backward()\n",
    "            optimizer.step() \n",
    "  \n",
    "        print(f'       ep{ep}:', torch.cuda.memory_allocated())\n",
    "        print(f'  reserved:', torch.cuda.memory_reserved())  \n",
    "        with torch.no_grad():\n",
    "\n",
    "            tlabel = net(k_train_data_tensor).reshape(k_train_label_tensor.shape[0])\n",
    "            vlabel = net(k_verify_data_tensor).reshape(k_verify_label_tensor.shape[0])\n",
    "            l1 = log_mse_loss(tlabel, k_train_label_tensor)\n",
    "            l2 = log_mse_loss(vlabel, k_verify_label_tensor)\n",
    "            train_loss_array[i][j] += l1\n",
    "            verify_loss_array[i][j] += l2\n",
    "\n",
    "            # animator.add([ep],[l1.to(\"cpu\"),l2.to(\"cpu\")])\n",
    "\n",
    "        # print(f\"l1 shape {l1.shape}: {l1}\")\n",
    "        # print(f'train label {tlabel[0:3]} \\nvs\\n{k_train_label_tensor[0:3]},\\nverify label {vlabel[0:3]} \\nvs \\n{k_verify_label_tensor[0:3]}')\n",
    "        \n",
    "        train_loss_array[i][j] += log_mse_loss(tlabel, k_train_label_tensor)\n",
    "        verify_loss_array[i][j] += log_mse_loss(vlabel, k_verify_label_tensor)\n",
    "        \n",
    "        \n",
    "\n",
    "# index is epochs#\n",
    "train_loss_mean = train_loss_array.sum(axis=0) / train_loss_array.shape[0]\n",
    "verify_loss_mean = verify_loss_array.sum(axis=0) / verify_loss_array.shape[0]\n",
    "# train_loss_array.shape, verify_loss_array.shape,  train_loss_mean, verify_loss_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plotting the training and validation loss\n",
    "plt.plot(np.arange(epochs), train_loss_mean, marker='o', label='Training Loss')\n",
    "plt.plot(np.arange(epochs), verify_loss_mean, marker='x', label='Validation Loss')\n",
    "\n",
    "# Adding title and labels\n",
    "plt.title('Training vs. Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()  # This adds a legend to distinguish between the two lines\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.cuda.memory_allocated())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(device)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean up GPU resource\n",
    "del train_data_tensor,train_label_tensor, net, optimizer, train_loss_array, verify_loss_array, train_loss_mean, verify_loss_mean\n",
    "torch.cuda.empty_cache()\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'After clean:', torch.cuda.memory_allocated())\n",
    "print(f'    reserved', torch.cuda.memory_reserved())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l-mine",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
